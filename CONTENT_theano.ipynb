{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2L_-P5CqisWn"
   },
   "source": [
    "## UTILITY FUNCTIONS  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OP8zroabibkH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def save_pkl(path, obj):\n",
    "  with open(path, 'w') as f:\n",
    "    cPickle.dump(obj, f)\n",
    "    print(\" [*] save %s\" % path)\n",
    "\n",
    "def load_pkl(path):\n",
    "  with open(path) as f:\n",
    "    obj = cPickle.load(f)\n",
    "    print(\" [*] load %s\" % path)\n",
    "    return obj\n",
    "\n",
    "def save_npy(path, obj):\n",
    "  np.save(path, obj)\n",
    "  print(\" [*] save %s\" % path)\n",
    "\n",
    "def load_npy(path):\n",
    "  obj = np.load(path)\n",
    "  print(\" [*] load %s\" % path)\n",
    "  return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbW8TplJi2bX"
   },
   "source": [
    "## DATASET CONSTRUCTOR CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Q9U81M8vifb_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "max_visit_size = 300\n",
    "\n",
    "\n",
    "class PatientReader(object):\n",
    "    def __init__(self, config):\n",
    "        self.data_path = data_path = config.data_path\n",
    "\n",
    "        self.vocab_path = vocab_path = os.path.join(data_path, \"vocab.pkl\")\n",
    "\n",
    "        # use train data to build vocabulary\n",
    "        if os.path.exists(vocab_path):\n",
    "            self._load()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_train_patients = math.ceil((len(self.X_train_data) + 0.0))\n",
    "        self.n_valid_patients = math.ceil((len(self.X_valid_data) + 0.0))\n",
    "        self.n_test_patients = math.ceil((len(self.X_test_data) + 0.0))\n",
    "\n",
    "        self.lda_vocab_size = config.lda_vocab_size\n",
    "        self.n_stops = config.n_stops\n",
    "\n",
    "        self.idx2word = {v: k for k, v in self.vocab.items()} #needed to go from index to concept \n",
    "\n",
    "        print(\"vocabulary size: {}\".format(self.vocab_size))\n",
    "        print(\"number of training documents: {}\".format(self.n_train_patients))\n",
    "        print(\"number of validation documents: {}\".format(self.n_valid_patients))\n",
    "        print(\"number of testing documents: {}\".format(self.n_test_patients))\n",
    "\n",
    "    def _load(self):\n",
    "        self.vocab = load_pkl(self.vocab_path)\n",
    "\n",
    "        self.X_train_data = load_pkl(self.data_path + '/' + 'X_train' + '.pkl')\n",
    "        self.Y_train_data = load_pkl(self.data_path + '/' + 'Y_train' + '.pkl')\n",
    "\n",
    "        self.X_valid_data = load_pkl(self.data_path + '/' + 'X_valid' + '.pkl')\n",
    "        self.Y_valid_data = load_pkl(self.data_path + '/' + 'Y_valid' + '.pkl')\n",
    "\n",
    "        self.X_test_data = load_pkl(self.data_path + '/' + 'X_test' + '.pkl')\n",
    "        self.Y_test_data = load_pkl(self.data_path + '/' + 'Y_test' + '.pkl')\n",
    "\n",
    "    def get_data_from_type(self, data_type):\n",
    "        if data_type == \"train\":\n",
    "            X_raw_data = self.X_train_data\n",
    "            Y_raw_data = self.Y_train_data\n",
    "        elif data_type == \"valid\":\n",
    "            X_raw_data = self.X_valid_data\n",
    "            Y_raw_data = self.Y_valid_data\n",
    "        elif data_type == \"test\":\n",
    "            X_raw_data = self.X_test_data\n",
    "            Y_raw_data = self.Y_test_data\n",
    "        else:\n",
    "            raise Exception(\" [!] Unkown data type %s: %s\" % data_type)\n",
    "\n",
    "        return X_raw_data, Y_raw_data\n",
    "\n",
    "    def get_Xc(self, data):\n",
    "        \"\"\"data is a patient...a sequence of visits\n",
    "            so a list of lists...the outer list is of size T_patient\n",
    "            the inner lists contain the concepts within each visit\n",
    "        \"\"\"\n",
    "        patient = [concept for visit in data for concept in visit]\n",
    "        patient = [x-1 for x in patient] \n",
    "        counts = np.bincount(patient, minlength=self.vocab_size)\n",
    "        stops_flag = np.array(list(np.ones([self.lda_vocab_size], dtype=np.int32)) +\n",
    "                              list(np.zeros([self.n_stops], dtype=np.int32)))\n",
    "\n",
    "        return counts * stops_flag\n",
    "\n",
    "    def get_X(self, data):\n",
    "        \"\"\"\n",
    "        data is a list of lists of different length\n",
    "        return an array of shape CxT where \n",
    "        entry Mij = ci if ci in visit j\n",
    "        \"\"\"\n",
    "        T_patient = len(data)\n",
    "        res = np.zeros([self.vocab_size, T_patient])\n",
    "        for i in range(self.vocab_size):\n",
    "            for j in range(T_patient):\n",
    "                if (i+1) in data[j]:\n",
    "                    res[i, j] = (i+1)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def iterator(self, data_type=\"train\"):\n",
    "        \"\"\"\n",
    "        goes over the data and\n",
    "        returns X, Xc, Y, and seq_len in a round robin\n",
    "        seq_len is a vector of size C where each \n",
    "        entry is T_patient\n",
    "        \"\"\"\n",
    "        X_raw_data, Y_raw_data = self.get_data_from_type(data_type)\n",
    "\n",
    "        x_infos = itertools.cycle(([self.get_X(X_doc[:max_visit_size]), self.get_Xc(X_doc[:max_visit_size])]\n",
    "                                   for X_doc in X_raw_data if X_doc != []))\n",
    "        y_infos = itertools.cycle(([Y_doc[:max_visit_size], np.array([len(Y_doc[:max_visit_size])]*self.vocab_size)]\n",
    "                                   for Y_doc in Y_raw_data if Y_doc != []))\n",
    "\n",
    "        return x_infos, y_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MiWwPUZi84_"
   },
   "source": [
    "## Config Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9UYRceTni3FP"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"feel free to play with these hyperparameters during training\"\"\"\n",
    "    dataset = \"resource\"  # change this to the right data name\n",
    "    data_path = \"../%s\" % dataset\n",
    "    checkpoint_dir = \"checkpoint\"\n",
    "    decay_rate = 0.95\n",
    "    decay_step = 1000\n",
    "    n_topics = 50\n",
    "    learning_rate = 0.00002\n",
    "    vocab_size = 619\n",
    "    n_stops = 22 \n",
    "    lda_vocab_size = vocab_size - n_stops\n",
    "    n_hidden = 200\n",
    "    n_layers = 2\n",
    "    projector_embed_dim = 100\n",
    "    generator_embed_dim = n_hidden\n",
    "    dropout = 1.0\n",
    "    max_grad_norm = 1.0 #for gradient clipping\n",
    "    total_epoch = 5\n",
    "    init_scale = 0.075\n",
    "    threshold = 0.5 #probability cut-off for predicting label to be 1\n",
    "    forward_only = False #indicates whether we are in testing or training mode\n",
    "\n",
    "    log_dir = '../logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQdSauR_jD-O"
   },
   "source": [
    "## Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-5_UcmL1jIoO",
    "outputId": "2b4ec5e7-baef-4ca9-aaa3-e2e70f70d731"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/Theano/Theano/archive/master.zip\n",
      "  Downloading https://github.com/Theano/Theano/archive/master.zip\n",
      "\u001b[K     / 13.4 MB 569 kB/s\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from Theano==1.0.5+unknown) (1.21.5)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Theano==1.0.5+unknown) (1.4.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from Theano==1.0.5+unknown) (1.15.0)\n",
      "Building wheels for collected packages: Theano\n",
      "  Building wheel for Theano (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for Theano: filename=Theano-1.0.5+unknown-py3-none-any.whl size=2667300 sha256=213d7e01acb54731e1e78add895eec23dc7578ac5db6a2c20698b3370190de86\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-db7beeqq/wheels/d1/38/85/558fc0e4d0c26db812304f163e348fb242767d3ea27da937fe\n",
      "Successfully built Theano\n",
      "Installing collected packages: Theano\n",
      "Successfully installed Theano-1.0.5+unknown\n",
      "Collecting https://github.com/Lasagne/Lasagne/archive/master.zip\n",
      "  Downloading https://github.com/Lasagne/Lasagne/archive/master.zip\n",
      "\u001b[K     \\ 231 kB 12.2 MB/s\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from Lasagne==0.2.dev1) (1.21.5)\n",
      "Building wheels for collected packages: Lasagne\n",
      "  Building wheel for Lasagne (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for Lasagne: filename=Lasagne-0.2.dev1-py3-none-any.whl size=122805 sha256=619e17cce887bdf0f7bf9ddb672a746a169bc17323082df9fdbe01aef1f60654\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8sj7_gaz/wheels/b6/a5/97/c657632d2b7fcff539623ea56996e09ec3c83c871e25a62cc5\n",
      "Successfully built Lasagne\n",
      "Installing collected packages: Lasagne\n",
      "Successfully installed Lasagne-0.2.dev1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade https://github.com/Theano/Theano/archive/master.zip\n",
    "!pip install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "HzAo5wkzi_AY",
    "outputId": "d90899c7-6ac3-43b2-8aa8-6dd3a6011ef2"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-23898c0e47e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimefusion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaskingLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mThetaLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lasagne.layers.timefusion'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from lasagne.layers.timefusion import MaskingLayer\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, accuracy_score, precision_recall_curve\n",
    "from lasagne.layers.theta import ThetaLayer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import average_precision_score as pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HPn7AFjjdmd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FR-replicate.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
